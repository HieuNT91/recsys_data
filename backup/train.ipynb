{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import dgl\n",
    "import random\n",
    "import numpy as np\n",
    "from dataloader import DataLoaderHGNN\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    seed = 123\n",
    "    task = 'rec'\n",
    "    data_name = 'douban_movie'\n",
    "    data_dir = 'data/'\n",
    "    use_pretrain = 0\n",
    "    pretrain_embedding_dir = 'datasets/pretrain/'\n",
    "    cf_batch_size = 90000\n",
    "    kg_batch_size = 10000\n",
    "    nd_batch_size = 5000\n",
    "    rl_batch_size = 1\n",
    "    train_batch_size = 2000\n",
    "    test_batch_size = 20000\n",
    "    entity_dim = 64\n",
    "    relation_dim = 32\n",
    "    aggregation_type = 'bi-interaction'\n",
    "    log = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(0)\n",
    "args=Args()\n",
    "infor = 'rl_' + str(args.data_name) + '_' + str(args.task) + '_' + str(args.log)\n",
    "model_name = 'model_' + infor + '.pth'\n",
    "dataset = args.data_name\n",
    "logger1 = get_logger('log', 'log/logger_' + infor + '.log')\n",
    "logger2 = get_logger('log2', 'log/logger2_' + infor + '.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoaderHGNN(logger1, args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_all_meta_path_from_subgraph():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieunt/anaconda3/envs/cfil/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        self.A1 = torch.nn.Linear(embedding_size)\n",
    "        self.A2 = torch.nn.Linear(embedding_size)\n",
    "        self.A3 = torch.nn.Linear(embedding_size)\n",
    "        \n",
    "    def mahalanobis_distance(self, prev_emb, next_emb, path_id):\n",
    "        if path_id == 0:\n",
    "            return (prev_emb - next_emb).T @ self.A1 @ (prev_emb - next_emb)\n",
    "        elif path_id == 1:\n",
    "            return (prev_emb - next_emb).T @ self.A2 @ (prev_emb - next_emb)\n",
    "        elif path_id == 2:\n",
    "            return (prev_emb - next_emb).T @ self.A3 @ (prev_emb - next_emb)\n",
    "        else:\n",
    "            raise NotImplementedError(f'wrong path id value: {path_id}')\n",
    "    \n",
    "    def mahalanobis_to_prob(self, prev_emb, next_emb, next_layers, path_id, use_log=True):\n",
    "\n",
    "        dist = self.mahalanobis_distance(prev_emb, next_emb, path_id)\n",
    "        normalizing_term = 0\n",
    "        for emb in next_layers:\n",
    "            normalizing_term += self.mahalanobis_distance(prev_emb, emb, path_id)\n",
    "        \n",
    "        if use_log == True:\n",
    "            return torch.log(dist / normalizing_term)\n",
    "        else:\n",
    "            return dist / normalizing_term\n",
    "\n",
    "    def criteria(y, prediction)    \n",
    "    def train(self, ):\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cfil')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21c731fbde3845cc416bfb5ae2926806a8b3329485f60478630f31d393cd0c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
